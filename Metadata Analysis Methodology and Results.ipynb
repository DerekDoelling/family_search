{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Family History Library Catalog Metadata Clean-up\"\n",
        "author: \"Talon Hintze, Sam Anderson, Karen Castillo, Dali Li, Z\"\n",
        "execute:\n",
        "  warning: false\n",
        "  fig.width: 5\n",
        "  fig.height: 5 \n",
        "format:\n",
        "  html:\n",
        "    theme: \"darkly\"\n",
        "    highlight: \"pygments\"\n",
        "    self-contained: true\n",
        "    page-layout: full\n",
        "    title-block-banner: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: body\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "    code-overflow: wrap\n",
        "    code-copy: hover\n",
        "    code-tools:\n",
        "        source: false\n",
        "        toggle: true\n",
        "        caption: See code\n",
        "---"
      ],
      "id": "3310e710"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "from lets_plot import *\n",
        "LetsPlot.setup_html()\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "raw = pl.read_excel(\"D:\\\\School\\\\Fall24\\\\Data Science Consulting\\\\xlsx data\\\\1-45k-A.xlsx\")\n",
        "\n",
        "data = pl.read_csv(\"D:\\\\School\\\\Fall24\\\\Data Science Consulting\\\\xlsx data\\\\combined.csv\", ignore_errors=True)\n",
        "data = data.with_row_index(name=\"index\")\n",
        "\n",
        "small = data.select([\"index\", \"040$b-Language of cataloging\", \"264$a-Place of production, publication, distribution, manufacture\", \"264$b-Name of producer, publisher, distributor, manufacturer\", \"264$c-Date of production, publication, or distribution\"])\n",
        "\n",
        "def remove_non_special_chars(df: pl.DataFrame, column_names: list) -> pl.DataFrame:\n",
        "    # Define the regex pattern to keep only the specified special characters\n",
        "    pattern = r\"[^@_!#$%^&*()<>?/\\|}{~:]\"\n",
        "    \n",
        "    # Loop through each column name in the provided list\n",
        "    for column_name in column_names:\n",
        "        # Apply the regex pattern to the column, replacing everything except special characters with an empty string\n",
        "        df = df.with_columns(\n",
        "            pl.col(column_name).str.replace_all(pattern, \"\").alias(column_name)\n",
        "        )\n",
        "    \n",
        "    return df\n",
        "\n",
        "def remove_numbers(df: pl.DataFrame, column_names:list) -> pl.DataFrame:\n",
        "    # Define the regex pattern for numbers (digits 0-9)\n",
        "    pattern = r\"\\d\"\n",
        "    \n",
        "    # Apply the regex pattern to the column, replacing numbers with an empty string\n",
        "    for column_name in column_names:\n",
        "\n",
        "        df = df.with_columns(\n",
        "            pl.col(column_name).str.replace_all(pattern, \"\").alias(column_name)\n",
        "        )\n",
        "    \n",
        "    return df"
      ],
      "id": "40e73392",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "The purpose of this document is to give an explanation of the methodologies used to achieve 3 main goals.\n",
        "\n",
        "1. Identify and compare record formats\n",
        "2. Compare various language columns\n",
        "3. Identify \n",
        "\n",
        "This document walks through the steps taken to acieve these goals with the intent that they can be replicated. They may also require some review by domain experts to ensure data is being handled effectively and appropriately. \n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "Data is exported in the MARC bibliographic format. This format is used by libraries to store metadata about books, journals, and other materials. Initially, the data looks something like this:\n"
      ],
      "id": "a8bf8f63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(raw.head())"
      ],
      "id": "590aaeff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unless someone is very familiar with MARC, these columns names don't mean anything significant. In an effor to make the data more understandable, we will rename the columns to something more meaningful. Columns were renamed according to the mark standard found [here](https://www.loc.gov/marc/bibliographic/).\n",
        "\n",
        "These column names were simply stored in a python dictionary and then mapped to their respective columns. This gives us a better understanding of what the data means with an output that looks like this:\n"
      ],
      "id": "27a8fec2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(data.head())"
      ],
      "id": "f44792dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Identifying and Analyzing Formats\n",
        "The Family History Library Catalog has hundreds of thousands of records. Unfortunately, these records through the years have conformed to many different formats. The goal of this project is to identify these format types and evaluate their similarity to each other.  This will help us to better understand the structure of the metadata in the Family History Library Catalog and give us more resources to employ in efforts to clean and standardize this metadata.\n",
        "\n",
        "For a proof of concept, let's take a look at column 264 and its related subfields (a, b, c). According to the Marc21 format, these fields have to do with publications information.\n",
        "\n",
        "* 264$a-Place of production, publication, distribution, manufacture\n",
        "* 264$b-Name of producer, publisher, distributor, manufacturer\n",
        "* 264$c-Date of production, publication, or distribution\n",
        "\n",
        "These fields have a good mix of letters, digits, and special characters. As such, they make for a good proof of concept. The methods and functions used here are designed to be scaled out to different columns. Keep in mind this is a rough draft and will be refined for production use.\n",
        "\n",
        "## Methodology\n",
        "After a visual analysis of the data found in this dataset (shown below), and after collaboration, it was determined that an effective way of identifying formatting patterns would be to remove any non special characters (letters, numbers, etc). This would leave behind only the special characters that are used for formatting.\n"
      ],
      "id": "0e78f7bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "small.head()"
      ],
      "id": "82608fe6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\n",
        "\n",
        "For example, if there is a value formatted like this: \"[city, state]\". It would now be represented as \"[, ]\". After formatting down to this format, each unique combination of special characters now represents a format type. This means that grouping by each of these unique combinations will give a count of how many records in your given sample pertain to that formatting standard.\n",
        "\n",
        "## 264$a-Place of production, publication, distribution, manufacture\n"
      ],
      "id": "80a4715b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "place = remove_non_special_chars(small, [\"264$a-Place of production, publication, distribution, manufacture\"]).select([\"index\", \"264$a-Place of production, publication, distribution, manufacture\"])\n",
        "\n",
        "place_agg = (\n",
        "    place\n",
        "    .group_by(\"264$a-Place of production, publication, distribution, manufacture\")\n",
        "    .agg(pl.len().alias(\"Count\"))  # Apply alias to the aggregation\n",
        ").sort(\"Count\", descending=True)"
      ],
      "id": "bd3dd2eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the first column of interest,  264$a. This column contains the place of publication, distribution, etc. After stripping out all but the special characters, we are left with data that looks like this.\n"
      ],
      "id": "298e013f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "place.drop_nulls().head(10)"
      ],
      "id": "e60d87ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\n",
        "\n",
        "Now, let's group by this column and count the number of records for each unique combination of special characters. Please note that characters from foreign languages are not currently being treated as traditional letter characters, so they will still appear.\n",
        "\n",
        "Here is a list of the top 10 formats.\n"
      ],
      "id": "97b7aed3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "place_agg.sort(\"Count\", descending=True).head(10)"
      ],
      "id": "d7ada7fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While a list of records is certainly useful, it can be hard to digest. Let's take a look at things in a more visual format using a simple bar chart. \n"
      ],
      "id": "594d27ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ggplot(place_agg, aes(x=\"264$a-Place of production, publication, distribution, manufacture\", y=\"Count\")) +\\\n",
        "    geom_bar(stat=\"identity\") +\\\n",
        "    labs(\n",
        "        title = \"Distribution of Publication Formats\",\n",
        "        x = \"Format of Place of Publication\",\n",
        "        y = \"# of Records\"\n",
        "    ) +\\\n",
        "    theme_minimal()"
      ],
      "id": "43e8e386",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing 264$a with 040$b-Language of cataloging\n",
        "\n",
        "The true value of this approach is to see what kinds of formats certain types of records have. To illustrate this let's take a look at the relationship between the language of cataloging and the formats present in column 264$a."
      ],
      "id": "d2721062"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df2 = (\n",
        "    small\n",
        "    # First, remove non-special characters in specified columns\n",
        "    .pipe(remove_non_special_chars, [\n",
        "        \"264$a-Place of production, publication, distribution, manufacture\"\n",
        "    ]).select([\"index\", \"264$a-Place of production, publication, distribution, manufacture\", '040$b-Language of cataloging'])\n",
        "    )\n",
        "\n",
        "test2 = (\n",
        "    df2\n",
        "    .group_by(\n",
        "        ['264$a-Place of production, publication, distribution, manufacture', \n",
        "         '040$b-Language of cataloging']\n",
        "    )\n",
        "    .agg(pl.len().alias(\"count\"))  # Ensure to name the count column\n",
        "    .pivot(\n",
        "        on='040$b-Language of cataloging', \n",
        "        index='264$a-Place of production, publication, distribution, manufacture', \n",
        "        values='count'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Convert to long format for plotting\n",
        "test_long2 = test2.unpivot(\n",
        "    index=['264$a-Place of production, publication, distribution, manufacture'],  # Keep this as identifier\n",
        "    on=test2.columns[1:],  # Use all other columns as value variables\n",
        "    variable_name='040$b-Language of cataloging',  # Name for the variable column\n",
        "    value_name='count'  # Name for the value column\n",
        ")\n",
        "\n",
        "ggplot(test_long2, aes(y=\"040$b-Language of cataloging\", x=\"264$a-Place of production, publication, distribution, manufacture\")) +\\\n",
        "    geom_tile(aes(fill=\"count\")) +\\\n",
        "    labs(title=\"264$a Formats by Language\",\n",
        "         x = \"264$a Format\",\n",
        "         y = \"Language Catalog\") +\\\n",
        "    theme_minimal() +\\\n",
        "    theme(plot_margin=(10, 80, 10, 10)) + \\\n",
        "    scale_fill_gradient(low = \"white\", high = \"blue\", limits = (0, 2200))"
      ],
      "id": "ff12fd43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This type of visualization can help us to understand which kinds of records tend to follow what kinds of formats. This knowledge will provide insight into certain types of records that will be easy and important to update. If the goal is to standardize formats across all records, a dynamic heatmap visualization like this can help to identify the best places to focus those efforts.\n",
        "\n",
        "# 2. Title and Text Languages\n",
        "\n",
        "To effectively utilize the bibliographic data from MARC 21, we aim to clean the dataset concerning titles and languages. This dataset includes fields such as: \n",
        "* '008 - Fixed-Length Data Elements - General Information,' \n",
        "* '040\\$b - Language of Cataloging,' \n",
        "* '041\\$a - Language Code of Text,' \n",
        "* '546\\$a - Language Note,' \n",
        "* '245\\$a - Title,' \n",
        "* '245\\$b - Remainder of Title,' \n",
        "* '245\\$c - Statement of Responsibility,' \n",
        "* '245\\$f - Inclusive Dates,' \n",
        "* '245\\$n,'\n",
        "* '245\\$p - Name of Part/Section of Work.' \n",
        "\n",
        "Currently, the information is dispersed across different columns, making it challenging to identify the correct data. The primary goal of this project is to organize the title and language columns to facilitate analysis by Family Search.\n",
        "\n",
        "## Language columns Clean\n",
        "\n",
        "The '008 - Fixed-Length Data Elements - General Information' field provides language information in positions 35 to 37. When multiple languages are indicated in the '008' field, only the '041\\$a - Language Code of Text' field is used to represent these languages. The combined '008' and '041' fields are used when multiple languages are present in '008,' as these languages are relevant for Family Search's purposes.\n",
        "\n",
        "The table shows that the top five languages used are English, French, German, Spanish, and Dutch.\n"
      ],
      "id": "25e00573"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% import the file\n",
        "df = data.to_pandas()\n",
        "col = df.columns.tolist()\n",
        "\n",
        "# %% drop null column\n",
        "df = df.loc[:, df.notnull().any()]\n",
        "\n",
        "# %% filter columns related to title and language\n",
        "df1 = df[['008-Fixed-Length Data Elements-General Information','040$b-Language of cataloging', '041$a-Language code of text','546$a-Language note', '245$a-Title', '245$b-Remainder of title', '245$c-Statement of responsibility','245$f-Inclusive dates', '245$n', '245$p-Name of part/section of work']]\n",
        "df1 = pd.DataFrame(df1)\n",
        "\n",
        "# %% split the 008 column by position 35-37\n",
        "df1['008-language'] = df1['008-Fixed-Length Data Elements-General Information'].str.slice(35, 38)\n",
        "\n",
        "# %% 008 column - langauge result\n",
        "result = df1.groupby('008-language').size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
        "\n",
        "# %% combined 008 and 041 only the case 008 have multi langauge. \n",
        "df1['008+041'] = np.where(pd.isna(df1['041$a-Language code of text']), \n",
        "                            df1['008-language'],  # Use '008-language' if '041$a-Language code of text' is NaN\n",
        "                            df1['041$a-Language code of text']) \n",
        "\n",
        "# %% split language column by ;\n",
        "df1_split2 = df1['008+041'].str.split(';', expand=True)\n",
        "df1_split2.columns = ['008+041_part1', '008+041_part2', '008+041_part3','008+041_part4','008+041_part5','008+041_part6']\n",
        "df1 = pd.concat([df1, df1_split2], axis=1)\n",
        "df1 = df1.fillna('None')\n",
        "\n",
        "# %% count of language code\n",
        "result1 = df1.groupby('008+041').size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
        "\n",
        "result1"
      ],
      "id": "1370b9a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Title columns Clean\n",
        "\n",
        "The 245$a - Title and 245$b - Remainder of Title fields display the title and subtitle. These fields were combined and then split by the delimiter '=' to create separate columns for each value, organizing the information effectively. The langid python library was used to determine the language used in the title. This library uses a different way to detect the lanague with MARC21. It was mapped to follow MARCH 21. This is reference: <href\\>https://www.loc.gov/marc/languages/language_c </href\\>\n",
        "\n",
        "The table illustrates the frequency of languages in the title column.\n"
      ],
      "id": "a9089b17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% combined 245a and 245b (title and subtitle)\n",
        "df1['245$ab'] = df1['245$a-Title'] + ' ' + df1['245$b-Remainder of title'].fillna('')\n",
        "\n",
        "# %% split 245$ab by determilter = and make columns for each splited values\n",
        "df1_split = df1['245$ab'].str.split('=', expand=True)\n",
        "df1_split.columns = ['245ab_part1', '245ab_part2', '245ab_part3', '245ab_part4', '245ab_part5']\n",
        "df1 = pd.concat([df1, df1_split], axis=1)\n",
        "\n",
        "#%% check contains = in 245$ab\n",
        "df1[df1['245$ab'].str.contains('=', na=False)]\n",
        "\n",
        "#%% langauage finder library\n",
        "import langid\n",
        "\n",
        "# %%\n",
        "# Function to detect the language\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        lang, _ = langid.classify(text)\n",
        "        return lang\n",
        "    except:\n",
        "        return 'None'\n",
        "\n",
        "# Apply the language detection to the new columns\n",
        "df1['245abpart1_lan1'] = df1['245ab_part1'].apply(detect_language)\n",
        "df1['245abpart2_lan2'] = df1['245ab_part2'].apply(detect_language)\n",
        "df1['245abpart3_lan3'] = df1['245ab_part3'].apply(detect_language)\n",
        "df1['245abpart4_lan4'] = df1['245ab_part4'].apply(detect_language)\n",
        "df1['245abpart5_lan5'] = df1['245ab_part5'].apply(detect_language)\n",
        "\n",
        "# %% count title column part : check on\n",
        "value_counts1 = df1['245abpart1_lan1'].value_counts()\n",
        "value_counts2 = df1['245abpart2_lan2'].value_counts()\n",
        "value_counts3 = df1['245abpart3_lan3'].value_counts()\n",
        "value_counts4 = df1['245abpart4_lan4'].value_counts()\n",
        "value_counts5 = df1['245abpart5_lan5'].value_counts()\n",
        "\n",
        "title_lan = pd.concat([value_counts1, value_counts2,value_counts3,value_counts4, value_counts5 ], axis=1)\n",
        "title_lan = title_lan.fillna(0)\n",
        "title_lan['Total'] = title_lan.sum(axis=1)\n",
        "title_lan"
      ],
      "id": "32c3ca6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %%\n",
        "# Language code to language name mapping\n",
        "language_mapping = {\n",
        "    'en': 'eng',  # English\n",
        "    'de': 'ger',  # German\n",
        "    'es': 'spa',  # Spanish\n",
        "    'fr': 'fre',  # French\n",
        "    'sv': 'swe',  # Swedish\n",
        "    'da': 'dan',  # Danish\n",
        "    'nl': 'dut',  # Dutch\n",
        "    'no': 'nor',  # Norwegian\n",
        "    'pt': 'por',  # Portuguese\n",
        "    'it': 'ita',  # Italian\n",
        "    'fi': 'fin',  # Finnish\n",
        "    'cs': 'cze',  # Czech\n",
        "    'gl': 'glg',  # Galician\n",
        "    'hu': 'hun',  # Hungarian\n",
        "    'la': 'lat',  # Latin\n",
        "    'id': 'ind',  # Indonesian\n",
        "    'pl': 'pol',  # Polish\n",
        "    'ms': 'may',  # Malay\n",
        "    'nn': 'nno',  # Norwegian (Nynorsk)\n",
        "    'sk': 'slo',  # Slovak\n",
        "    'is': 'ice',  # Icelandic\n",
        "    'af': 'afr',  # Afrikaans\n",
        "    'cy': 'wel',  # Welsh\n",
        "    'vo': 'vol',  # Volapük\n",
        "    'ca': 'cat',  # Catalan\n",
        "    'ro': 'rum',  # Romanian\n",
        "    'lt': 'lit',  # Lithuanian\n",
        "    'nb': 'nob',  # Norwegian (Bokmål)\n",
        "    'eu': 'baq',  # Basque\n",
        "    'sw': 'swa',  # Swahili\n",
        "    'hr': 'hrv',  # Croatian\n",
        "    'fo': 'fao',  # Faroese\n",
        "    'et': 'est',  # Estonian\n",
        "    'sl': 'slv',  # Slovenian\n",
        "    'mg': 'mlg',  # Malagasy\n",
        "    'lv': 'lav',  # Latvian\n",
        "    'ga': 'gle',  # Irish\n",
        "    'tr': 'tur',  # Turkish\n",
        "    'qu': 'que',  # Quechua\n",
        "    'tl': 'tgl',  # Tagalog\n",
        "    'jv': 'jav',  # Javanese\n",
        "    'ja': 'jpn',  # Japanese\n",
        "    'lb': 'ltz',  # Luxembourgish\n",
        "    'eo': 'epo',  # Esperanto\n",
        "    'xh': 'xho',  # Xhosa\n",
        "    'rw': 'kin',  # Kinyarwanda\n",
        "    'mt': 'mlt',  # Maltese\n",
        "    'an': 'arg',  # Aragonese\n",
        "    'ru': 'rus',  # Russian\n",
        "    'hy': 'arm',  # Armenian\n",
        "    'oc': 'oci',  # Occitan (post-1500)\n",
        "    'bg': 'bul',  # Bulgarian\n",
        "    'se': 'sme',  # Northern Sami\n",
        "    'ht': 'hat',  # Haitian French Creole\n",
        "    'wa': 'wln',  # Walloon\n",
        "    'zh': 'chi'   # Chinese\n",
        "}\n",
        "\n",
        "# %% apply MARC definitions to title \n",
        "df1['245abpart1_lan1'] = df1['245abpart1_lan1'].replace(language_mapping)\n",
        "df1['245abpart2_lan2'] = df1['245abpart2_lan2'].replace(language_mapping)\n",
        "df1['245abpart3_lan3'] = df1['245abpart3_lan3'].replace(language_mapping)\n",
        "df1['245abpart4_lan4'] = df1['245abpart4_lan4'].replace(language_mapping)\n",
        "df1['245abpart5_lan5'] = df1['245abpart5_lan5'].replace(language_mapping)"
      ],
      "id": "983aea5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of Language and Title\n",
        "\n",
        "We identified four pattersn between the language and title:\n",
        "1. Language and title match.\n",
        "2. Cases with multiple languages in the language column but not in the title.\n",
        "3. Cases with multiple languages in the title but not in the language column.\n",
        "4. Cases where languages differ between the language and title columns.\n"
      ],
      "id": "51e304c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of the columns you're interested in\n",
        "title_cols= ['245abpart1_lan1', '245abpart2_lan2', '245abpart3_lan3', '245abpart4_lan4', '245abpart5_lan5']\n",
        "lan_cols = ['008+041_part1', '008+041_part2', '008+041_part3', '008+041_part4', '008+041_part5']\n",
        "# %%\n",
        "def compare_columns(row):\n",
        "    matching_values = []\n",
        "    title_not_matching = []\n",
        "    lan_not_matching = []\n",
        "    \n",
        "    # Iterate over corresponding column pairs\n",
        "    for title_col, lan_col in zip(title_cols, lan_cols):\n",
        "        title_value = row.get(title_col, '')\n",
        "        lan_value = row.get(lan_col, '')\n",
        "\n",
        "        if isinstance(title_value, str) and isinstance(lan_value, str):\n",
        "            if title_value == lan_value and title_value:\n",
        "                matching_values.append(title_value)\n",
        "            else:\n",
        "                title_not_matching.append(title_value)\n",
        "                lan_not_matching.append(lan_value)\n",
        "    \n",
        "    # Prepare the results\n",
        "    matching_value_result = ', '.join(matching_values) if matching_values else 'Unmatched'\n",
        "    title_not_matching_result = ', '.join(title_not_matching) if title_not_matching else 'Matched'\n",
        "    lan_not_matching_result = ', '.join(lan_not_matching) if lan_not_matching else 'Matched'\n",
        "    \n",
        "    return matching_value_result, title_not_matching_result, lan_not_matching_result\n",
        "\n",
        "# %%\n",
        "# Apply the function across the DataFrame and expand results into new columns\n",
        "df1[['matching_value', 'language_245_not_matching', 'language_008+041_not_matching']] = df1.apply(compare_columns, axis=1, result_type='expand')\n",
        "# %%\n",
        "# Clean the 'matching_value' column and remove 'None' entries\n",
        "df1['matching_value'] = df1['matching_value'].apply(lambda x: ', '.join(value.strip() for value in x.split(',') if value.strip() != 'None'))\n",
        "# %%\n",
        "# Clean up 'language_245_not_matching' and 'language_008+041_not_matching'\n",
        "def clean_none(value):\n",
        "    return ', '.join([lang for lang in value.split(', ') if lang.strip() != 'None']) if value else 'None'\n",
        "\n",
        "df1['language_245_not_matching'] = df1['language_245_not_matching'].apply(clean_none).fillna('None').replace('', 'None')\n",
        "df1['language_008+041_not_matching'] = df1['language_008+041_not_matching'].apply(clean_none).fillna('None').replace('', 'None')\n",
        "\n",
        "# Create a column to check if both 'language_245' and 'language_008+041' are matching\n",
        "df1['both_matching'] = (df1['language_245_not_matching'] == 'Matched') & (df1['language_008+041_not_matching'] == 'Matched')"
      ],
      "id": "b8dec726",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results \n",
        "We found that most cases fall under Case 1, where both the title and language records match, totaling 6,471 instances. In these cases, multiple languages are present in the record, not just in the title. Case 3, with 74 instances, has multiple languages in the title that are not reflected in the language column. Case 4, with 11,442 instances, shows different languages between the title and language column.\n",
        "\n",
        "Cases 3 and 4 require careful consideration to understand why these discrepancies occur. In particular, case 4 may involve errors, as the Python library might not accurately detect the correct language from the title.\n"
      ],
      "id": "9ff9432d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% Find the cases\n",
        "filtered = df1[['245$a-Title','matching_value', 'language_245_not_matching', 'language_008+041_not_matching']]\n",
        "\n",
        "case1 = filtered[(filtered['language_245_not_matching'] == \"Matched\") & (filtered['language_008+041_not_matching'] == \"Matched\")]\n",
        "case2 = filtered[filtered['language_245_not_matching']== \"None\" ] \n",
        "case3 = filtered[filtered['language_008+041_not_matching']== \"None\" ] \n",
        "case4 = filtered[filtered['matching_value']== \"\" ] \n",
        "\n",
        "# %% final result\n",
        "# Count rows in each case\n",
        "counts = {\n",
        "    \"Case 1\": case1.shape[0],\n",
        "    \"Case 2\": case2.shape[0],\n",
        "    \"Case 3\": case3.shape[0],\n",
        "    \"Case 4\": case4.shape[0]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "counts_df = pd.DataFrame(list(counts.items()), columns=['Case', 'Count'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(counts_df['Case'], counts_df['Count'], color='skyblue')\n",
        "\n",
        "# Add counts on top of each bar\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel(\"Case\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Number of Rows per Case\")\n",
        "plt.show()\n",
        "\n",
        "# %% save as excel\n",
        "# with pd.ExcelWriter('output_cases.xlsx') as writer:\n",
        "#     df1.to_excel(writer, sheet_name='All', index=True)\n",
        "#     case1.to_excel(writer, sheet_name='Case1', index=True)\n",
        "#     case2.to_excel(writer, sheet_name='Case2', index=True)\n",
        "#     case3.to_excel(writer, sheet_name='Case3', index=True)\n",
        "#     case4.to_excel(writer, sheet_name='Case4', index=True)"
      ],
      "id": "9d73249c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Date Pattern Analysis\n",
        "This analysis examines entries in the 245$f-Inclusive dates field of MARC records, categorizing them based on specific date patterns. The workflow consists of:\n",
        "\n",
        "* Initial Data Inspection: Conducts an initial exploration of the dataset, including missing value identification.\n",
        "\n",
        "* Date Pattern Categorization: Groups date entries by common formats such as ranges and single years.\n",
        "\n",
        "* Data Cleaning: Removes special characters and analyzes the cleaned dataset for consistency.\n",
        "\n",
        "* Special Character Analysis: Counts and assesses the presence of special characters.\n",
        "\n",
        "* Detailed 'Other' Pattern Breakdown: Categorizes complex date formats in the \"Other\" group.\n",
        "\n",
        "## Initial Data Exploration\n",
        "We start by loading the data and exploring the first few rows of the 245$f-Inclusive dates column to understand its structure and identify missing values.\n"
      ],
      "id": "130e561d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the data\n",
        "df = data.to_pandas()\n",
        "\n",
        "# Step 1: Initial exploration of the data\n",
        "initial_data_df = df[['245$f-Inclusive dates']].head(10)\n",
        "missing_values_count = df['245$f-Inclusive dates'].isnull().sum()\n",
        "unique_values = df['245$f-Inclusive dates'].unique()[:10]\n"
      ],
      "id": "0fb66fb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "* The dataset includes missing values and diverse date formats.\n",
        "* Initial rows and missing values are summarized below:\n"
      ],
      "id": "f5f41253"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display initial exploration results\n",
        "exploration_summary = pd.DataFrame({\n",
        "    \"Metric\": [\"Missing Values\", \"Unique Values (First 10)\"],\n",
        "    \"Value\": [missing_values_count, unique_values]\n",
        "})\n",
        "display(HTML(\"<h3>Summary of Missing and Unique Values</h3>\"))\n",
        "display(HTML(exploration_summary.to_html(index=False)))"
      ],
      "id": "d744286d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorizing Date Patterns\n",
        "We classify dates into four categories:\n",
        "\n",
        "* Missing: No date provided.\n",
        "* Date Range: A range in the format YYYY-YYYY.\n",
        "* Single Year: A single year in the format YYYY.\n",
        "* Other: Any other format.\n"
      ],
      "id": "0ff4873b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Categorize date patterns\n",
        "def categorize_date_pattern(date):\n",
        "    if pd.isnull(date):\n",
        "        return 'Missing'\n",
        "    elif re.match(r'^\\d{4}-\\d{4}$', date):\n",
        "        return 'Date Range'\n",
        "    elif re.match(r'^\\d{4}$', date):\n",
        "        return 'Single Year'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['date_pattern'] = df['245$f-Inclusive dates'].apply(categorize_date_pattern)\n",
        "\n",
        "# Count occurrences of each pattern\n",
        "date_pattern_counts = df['date_pattern'].value_counts()\n",
        "total_date_patterns = date_pattern_counts.sum()\n",
        "date_pattern_df = pd.DataFrame({\n",
        "    'Date Pattern': date_pattern_counts.index,\n",
        "    'Count': date_pattern_counts.values,\n",
        "    'Percentage': (date_pattern_counts / total_date_patterns * 100).round(2)\n",
        "})"
      ],
      "id": "6ab78676",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of Date Patterns\n",
        "The table below summarizes the distribution of date patterns:\n"
      ],
      "id": "eb6ea625"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display pattern distribution\n",
        "display(HTML(\"<h3>Date Pattern Distribution</h3>\"))\n",
        "display(HTML(date_pattern_df.to_html(index=False)))"
      ],
      "id": "34dcb3fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Date Patterns\n",
        "The distribution of date patterns is shown in the bar chart below. Annotations highlight examples for each category.\n"
      ],
      "id": "f1fd5e42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Visualize date patterns\n",
        "plt.figure(figsize=(10, 5))\n",
        "bars = plt.bar(date_pattern_counts.index, date_pattern_counts.values, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Date Pattern Category')\n",
        "plt.ylabel('Count of Records')\n",
        "plt.title('Distribution of Date Patterns in 245$f-Inclusive dates')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add annotations with examples\n",
        "annotations = {\n",
        "    'Missing': 'Examples:\\nNaN\\n...',\n",
        "    'Date Range': 'Examples:\\n\"1549-1802\",\\n\"1675-1811\"',\n",
        "    'Single Year': 'Examples:\\n\"1703\",\\n\"1905\"',\n",
        "    'Other': 'Examples:\\n\"1585-1624, 1768-1804\",\\n\"1624-1840 :\"'\n",
        "}\n",
        "for i, bar in enumerate(bars):\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 500, annotations[date_pattern_counts.index[i]], ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.show()"
      ],
      "id": "e27086d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Special Character Analysis\n",
        "We analyze the cleaned data by removing non-special characters to focus on formatting.\n"
      ],
      "id": "6c1f3bcc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Remove non-special characters\n",
        "def remove_non_special_chars(df, column_name):\n",
        "    pattern = r\"[^@_!#$%^&*()<>?/\\|}{~:]\"\n",
        "    df[column_name] = df[column_name].str.replace(pattern, \"\", regex=True)\n",
        "    return df\n",
        "\n",
        "df['245$f-Cleaned'] = df['245$f-Inclusive dates']\n",
        "df = remove_non_special_chars(df, '245$f-Cleaned')\n",
        "\n",
        "# Step 5: Count occurrences of special characters\n",
        "def count_special_characters(column_data):\n",
        "    char_counts = {}\n",
        "    special_char_pattern = re.compile(r\"[@_!#$%^&*()<>?/\\|}{~:]+\")\n",
        "    for entry in column_data.dropna():\n",
        "        matches = special_char_pattern.findall(entry)\n",
        "        for match in matches:\n",
        "            char_counts[match] = char_counts.get(match, 0) + 1\n",
        "    char_df = pd.DataFrame(list(char_counts.items()), columns=['Character Sequence', 'Count']).sort_values(by='Count', ascending=False)\n",
        "    char_df['Percentage'] = (char_df['Count'] / char_df['Count'].sum() * 100).round(2)\n",
        "    return char_df\n",
        "\n",
        "char_df = count_special_characters(df['245$f-Cleaned'])"
      ],
      "id": "12be55c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "Below is a table of the most frequent special characters and their percentages:\n"
      ],
      "id": "72f79bd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display special character analysis\n",
        "display(HTML(\"<h3>Special Character Analysis</h3>\"))\n",
        "display(HTML(char_df.to_html(index=False)))"
      ],
      "id": "6853a68c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Analysis of 'Other' Patterns\n",
        "Within the \"Other\" category, we perform an additional breakdown to identify sub-patterns."
      ],
      "id": "e505d81b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 6: Analyze 'Other' patterns\n",
        "def categorize_other_pattern(date):\n",
        "    if re.match(r'^\\d{4}-\\d{4},\\s?\\d{4}-\\d{4}$', date):\n",
        "        return 'Date Range with Commas'\n",
        "    elif re.match(r'^\\d{1,2}/\\d{1,2}/\\d{4}\\s\\d{1,2}:\\d{2}', date):\n",
        "        return 'Datetime Format'\n",
        "    elif re.match(r'^\\d{4}-\\d{4}\\s?[.:]$', date):\n",
        "        return 'Date Range with End Punctuation'\n",
        "    elif re.match(r'^\\d{4},\\s?\\d{4}$', date):\n",
        "        return 'Single Years with Comma'\n",
        "    else:\n",
        "        return 'Other Unspecified Pattern'\n",
        "\n",
        "df_other = df[df['date_pattern'] == 'Other'].copy()\n",
        "df_other['Other_pattern'] = df_other['245$f-Inclusive dates'].apply(categorize_other_pattern)\n",
        "\n",
        "# Summarize 'Other' patterns\n",
        "other_pattern_counts = df_other['Other_pattern'].value_counts()\n",
        "detailed_other_df = pd.DataFrame({\n",
        "    'Other Pattern': other_pattern_counts.index,\n",
        "    'Count': other_pattern_counts.values\n",
        "})"
      ],
      "id": "41405190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 'Other' Pattern Summary"
      ],
      "id": "c747343f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display detailed 'Other' pattern breakdown\n",
        "display(HTML(\"<h3>Detailed 'Other' Pattern Breakdown</h3>\"))\n",
        "display(HTML(detailed_other_df.to_html(index=False)))"
      ],
      "id": "f851491e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "This analysis provides insights into date formatting within MARC records. It identifies common patterns and highlights opportunities for data standardization. The cleaned and categorized data can be exported for further use.\n",
        "\n",
        "## Date Pattern Analysis Based on Record Type\n",
        "This section categorizes date patterns and creates a contingency table to analyze the relationship between publication status and date patterns.\n",
        "\n",
        "### Step 1: Categorize Date Patterns\n",
        "Date patterns in the 245$f-Inclusive dates field are categorized into four groups: Missing, Date Range, Single Year, and Other.\n"
      ],
      "id": "633aeb02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Redefine df_combined as df (if no modifications are needed from preprocessing)\n",
        "df_combined = df\n",
        "\n",
        "# Redefine df_combined as df\n",
        "df_combined = df\n",
        "\n",
        "# Step 1: Categorize Date Patterns\n",
        "def categorize_date_pattern(date):\n",
        "    if pd.isnull(date):\n",
        "        return 'Missing'\n",
        "    elif re.match(r'^\\d{4}-\\d{4}$', date):\n",
        "        return 'Date Range'\n",
        "    elif re.match(r'^\\d{4}$', date):\n",
        "        return 'Single Year'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "# Apply the function to the '245$f-Inclusive dates' column in df_combined\n",
        "df_combined['date_pattern'] = df_combined['245$f-Inclusive dates'].apply(categorize_date_pattern)\n",
        "\n",
        "# Step 2: Recreate 'Publication Status' if needed\n",
        "if '008-Fixed-Length Data Elements-General Information' in df_combined.columns:\n",
        "    df_combined['Publication Status'] = df_combined['008-Fixed-Length Data Elements-General Information'].str[6]\n",
        "else:\n",
        "    print(\"'Publication Status' column could not be generated because '008-Fixed-Length Data Elements-General Information' is missing.\")"
      ],
      "id": "5b70dc4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create Contingency Table\n",
        "A contingency table shows the distribution of date patterns across different publication statuses.\n"
      ],
      "id": "cd775771"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Create a contingency table\n",
        "if 'Publication Status' in df_combined.columns and 'date_pattern' in df_combined.columns:\n",
        "    pattern_counts = df_combined.groupby(['Publication Status', 'date_pattern']).size().unstack(fill_value=0)\n",
        "    print(\"Exact Counts of Date Patterns for Each Record Type:\")\n",
        "    print(tabulate(pattern_counts, headers='keys', tablefmt='psql'))\n",
        "else:\n",
        "    print(\"Either 'Publication Status' or 'date_pattern' column is missing. Contingency table cannot be generated.\")"
      ],
      "id": "afbc70d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Leader Bilbliography and Child/Parent Analysis\n",
        "\n",
        "I Identify incorrect entries in: \n",
        "- 008-Leader column (7th place in cell which indicates bibliography)\n",
        "\n",
        "- 773$w column (code of the parent record)\n",
        "\n",
        "- 336-338 columns with Record type info\n",
        "\n",
        "## Introduction\n",
        "The book records catelog at family search have records indexed throughout the years with different standards of indexing, and may also contain mistakes. My goal is to analyize records that do not meet the MARC standards, or have invalid values, or contridictory entries.\n",
        "\n",
        "## 000-Leader column (7th places which indicates bibliography)\n",
        "\n",
        "The column each place in the \"000-Leader\" represents different information. The 7th place contains the Bibliographic level. Valid letters are 'a', 'm', 's', 'b'.\n",
        "\n",
        "A - Monographic component part\n",
        "\n",
        "b - Serial component part\n",
        "\n",
        "c - collection\n",
        "\n",
        "d - Subunit\n",
        "\n",
        "i - Intergrating resource\n",
        "\n",
        "m - Monograph/item\n",
        "\n",
        "s - Serial\n",
        "\n",
        "Invalid entries are - t, - k, - e,- i,- g,- d. If we look at the rows with invalid values in the 7th columnm, and then look at the 8th column, we see valid entries that should be in the 7th place.\n",
        "Most likely there was an issue during exporting the data that caused some of the entries to be off by one place value.\n"
      ],
      "id": "da105138"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Define file path and read CSV with error handling\n",
        "csv_file_path = 'D:\\\\School\\\\Fall24\\\\Data Science Consulting\\\\xlsx data\\\\combined.csv'\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path, on_bad_lines='skip', low_memory=False)  # Handles bad lines gracefully\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# Step 2: Clean data by removing columns with only NA values\n",
        "df_cleaned = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Step 3: Filter columns with specific prefixes\n",
        "prefixes = [\n",
        "    '000','001', '008', '245$6', '245$a', '245$b', '245$f', '245$n', '245$p',\n",
        "    '260$a', '260$b', '260$c',\n",
        "    '264$a', '264$b', '264$c', '773$w'\n",
        "]\n",
        "df_filtered = df_cleaned[[col for col in df_cleaned.columns if any(col.startswith(prefix) for prefix in prefixes)]]\n",
        "\n",
        "# Step 4: Split '008-Fixed-Length Data Elements-General Information' into separate columns\n",
        "if '008-Fixed-Length Data Elements-General Information' in df_filtered.columns:\n",
        "    df_split = df_filtered['008-Fixed-Length Data Elements-General Information'].apply(\n",
        "        lambda x: pd.Series({\n",
        "            'Record Creation Date': x[:6] if len(x) >= 6 else np.nan,\n",
        "            'Publication Status': x[6] if len(x) >= 7 else np.nan,\n",
        "            'Date 1': x[7:11] if len(x) >= 11 else np.nan,\n",
        "            'Date 2': x[11:15] if len(x) >= 15 else np.nan,\n",
        "            'Place of Publication': x[15:18] if len(x) >= 18 else np.nan,\n",
        "            'Language': x[35:38] if len(x) >= 38 else np.nan,\n",
        "            'Modified Record': x[38] if len(x) >= 39 else np.nan\n",
        "        })\n",
        "    )\n",
        "    df_combined = pd.concat([df_filtered, df_split], axis=1)\n",
        "else:\n",
        "    df_combined = df_filtered\n",
        "\n",
        "# Step 5: Add 'Bibliography' column from '000-Leader'\n",
        "df_combined['Bibliography'] = df_combined['000-Leader'].str[6]  # 7th character is at index 6\n",
        "df_combined['6th'] = df_combined['000-Leader'].str[5]  # 6th character (index starts from 0)\n",
        "df_combined['8th'] = df_combined['000-Leader'].str[7]  # 8th character\n",
        "\n",
        "# Step 6: Filter rows based on 'Bibliography' values\n",
        "exclude_chars = ['a', 'm', 's', 'b']\n",
        "filtered_df = df_combined[~df_combined['Bibliography'].isin(exclude_chars)]\n",
        "\n",
        "# Step 7: Select specific columns and add new columns from '000-Leader'\n",
        "result_df = filtered_df[['000-Leader', 'Bibliography', '6th', '8th']].copy()\n",
        "\n",
        "# Step 8: Count values for specific columns (excluding the '6th' column)\n",
        "def print_counts(column_name):\n",
        "    value_counts = result_df[column_name].value_counts()\n",
        "    print(f\"\\nCount of each distinct value in the {column_name} column:\")\n",
        "    print(tabulate(value_counts.reset_index().values, headers=['Value', 'Count'], tablefmt='pretty'))\n",
        "\n",
        "for col in ['Bibliography', '8th']:\n",
        "    print_counts(col)\n",
        "\n",
        "# Step 9: Check if total count of 'Bibliography' matches total count of '8th' column\n",
        "bibliography_total_count = result_df['Bibliography'].count()\n",
        "eighth_total_count = result_df['8th'].count()\n",
        "print(f\"\\nTotal count in Bibliography column: {bibliography_total_count}\")\n",
        "print(f\"Total count in 8th character column: {eighth_total_count}\")\n",
        "if bibliography_total_count == eighth_total_count:\n",
        "    print(\"The counts match.\")\n",
        "else:\n",
        "    print(\"The counts do not match.\")"
      ],
      "id": "ff359196",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 10: Export filtered rows to a CSV file with '6th' and '8th' columns from result_df, including '001-Control Number'\n",
        "output_csv_path = '/content/filtered_bibliography.csv'\n",
        "filtered_export_df = filtered_df.merge(result_df[['000-Leader', '6th', '8th']], on='000-Leader', how='left')\n",
        "\n",
        "desired_columns = ['000-Leader', 'Bibliography', '6th', '8th', '001-Control Number']\n",
        "available_columns = [col for col in desired_columns if col in filtered_export_df.columns]\n",
        "filtered_export_df = filtered_export_df[available_columns]\n",
        "\n",
        "# Export to CSV\n",
        "#filtered_export_df.to_csv(output_csv_path, index=False)\n",
        "#print(f\"\\nFiltered rows exported to {output_csv_path}\")"
      ],
      "id": "448cb61e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 008-Fixed-Length Data Elements-General Information\n",
        "This column contains all of the following in one column:\n",
        " -Record Creation Date, \n",
        " -Publication Status,\n",
        " -Date 1,\n",
        " -Date 2,\n",
        " -Place of Publication,\n",
        " -Language,\n",
        " -Modified Record\n",
        " *There is space for other information for records pertaining to maps, music, and a few other things but those things are not in the dataset\n",
        "\n",
        " I split the column into 7 column, by the fields noted above.\n"
      ],
      "id": "f78910cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 11: Stats on '008-Fixed-Length Data Elements'\n",
        "# Step 11.1: Filter to only include specific columns\n",
        "df_combined_filtered = df_combined[[\n",
        "    '008-Fixed-Length Data Elements-General Information',\n",
        "    'Record Creation Date',\n",
        "    'Publication Status',\n",
        "    'Date 1',\n",
        "    'Date 2',\n",
        "    'Place of Publication',\n",
        "    'Language',\n",
        "    'Modified Record'\n",
        "]].copy()\n",
        "\n",
        "# Step 11.2: Create a new DataFrame that excludes specific values in 'Publication Status'\n",
        "exclude_values = ['b', 'c', 'd', 'i', 'k', 'm', 'n']\n",
        "df_combined_filtered = df_combined_filtered[~df_combined_filtered['Publication Status'].isin(exclude_values)].copy()\n",
        "\n",
        "# Step 11.3: Show count of distinct values for 'Publication Status' and 'Language'\n",
        "value_counts_publication_status = df_combined_filtered['Publication Status'].value_counts()\n",
        "print(\"\\nCount of each distinct value in the 'Publication Status' column:\")\n",
        "print(tabulate(value_counts_publication_status.reset_index().values, headers=['Publication Status', 'Count'], tablefmt='pretty'))\n",
        "\n",
        "value_counts_language = df_combined_filtered['Language'].value_counts()\n",
        "#print(\"\\nCount of each distinct value in the 'Language' column:\")\n",
        "#print(tabulate(value_counts_language.reset_index().values, headers=['Language', 'Count'], tablefmt='pretty'))"
      ],
      "id": "5d289208",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rows with invalid entries for the bibliographic info (7th character in the column) all have a valid entry but happens to be in the 8th space. The same amount of invalid entries in the 7th space equals the same amount of entries (which are valid) in the 8th space.\n",
        "\n",
        "## 773$w column (code of the parent record)\n",
        "\n",
        "773$w column contains the parent record code. We can match the 773$w with the 0001-Leader column to find the parent record row. The 773$w column is empty for many columns, but let's look at how many parent-child records match up\n"
      ],
      "id": "87781bb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 12: Analysis on '773$w' and related columns\n",
        "# Step 12.1: Filter rows where '773$w' is not null\n",
        "filtered_df_773w = df_combined[df_combined['773$w'].notna()]\n",
        "\n",
        "# Step 12.2: Select only the desired columns\n",
        "filtered_columns_df = filtered_df_773w[['000-Leader', '001-Control Number', '773$w', 'Bibliography', '8th']]\n",
        "\n",
        "#filtered_columns_df\n",
        "#28 rows have data in the 773$w column"
      ],
      "id": "1e8a8935",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 12.3: Drop NA values from '773$w' to focus on actual values\n",
        "values_in_773w = df_combined['773$w'].dropna()\n",
        "\n",
        "# Step 12.4: Filter rows where '001-Control Number' matches a value in '773$w'\n",
        "matching_rows = df_combined[df_combined['001-Control Number'].isin(values_in_773w)]\n",
        "\n",
        "# Step 12.5: Create a new column 'Parent Control Number' in df_combined to store the matched '001-Control Number' values\n",
        "df_combined['Parent Control Number'] = df_combined['773$w'].apply(lambda x: x if x in matching_rows['001-Control Number'].values else np.nan)\n",
        "\n",
        "# Step 12.6: Filter and display the columns of interest where 'Parent Control Number' is not NaN\n",
        "matched_df = df_combined[df_combined['Parent Control Number'].notna()][['000-Leader', '001-Control Number', '773$w', 'Parent Control Number', '245$a-Title']]\n",
        "print(\"\\nChild Records with existing Parent Records:\")\n",
        "matched_df"
      ],
      "id": "9f4ba6ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 12.7: Filter rows where '773$w' is not in '001-Control Number'\n",
        "unmatched_df = df_combined[df_combined['773$w'].isin(values_in_773w) & ~df_combined['773$w'].isin(df_combined['001-Control Number'])]\n",
        "\n",
        "# Step 12.8: Select and display the desired columns\n",
        "unmatched_df_filtered = unmatched_df[['000-Leader', '001-Control Number', '773$w', 'Parent Control Number','245$a-Title']]\n",
        "print(\"\\nChild Records without exisiting Parent Records in data :\")\n",
        "unmatched_df_filtered"
      ],
      "id": "a5c0c086",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aboe are the records that we are missing the Parent records. Those records will need to be found, as they are not in our dataset.\n"
      ],
      "id": "795ae755"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 12.9: Export unmatched rows to a CSV file\n",
        "#unmatched_output_csv_path = '/content/unmatched_rows.csv'\n",
        "#unmatched_df_filtered.to_csv(unmatched_output_csv_path, index=False)\n",
        "#print(f\"\\nUnmatched rows exported to {unmatched_output_csv_path}\")"
      ],
      "id": "3b9dfbf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 337 through 338, columns with Record type info\n",
        "\n",
        "These columns contain information on the record type of the books, which include if there is text, if it is a phsyical or eletronic record, and how many pages/volumes/leaves. I will look at if all the columns for any given row suggests there are the same amount of records.\n",
        "\n",
        "One record can have multiple record types, and/or show that there are multiple volumes of a work.\n"
      ],
      "id": "7c96f375"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 13: Analysis on '336$2' and related columns\n",
        "# Step 13.1: Filter rows where '336$2' is not null\n",
        "filtered_df_336 = df_cleaned[df_cleaned['336$2'].notna()]\n",
        "\n",
        "# Step 13.2: Select only the desired columns\n",
        "filtered_columns_df_336 = filtered_df_336[['000-Leader','300$a-Extent', '300$b-Other physical details', '300$c-Dimensions', '310$a',\n",
        "                                           '336$2', '336$a', '336$b', '337$2', '337$a', '337$b',\n",
        "                                           '338$2', '338$a', '338$b', '362$a-Start date of publication']]\n",
        "print(\"\\nFiltered DataFrame with non-null '336$2':\")\n",
        "filtered_columns_df_336"
      ],
      "id": "38a2e829",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 13.3: Display distinct values in specified columns\n",
        "distinct_values = {}\n",
        "columns_to_check = ['336$2', '336$a', '336$b', '337$2', '337$a', '337$b', '338$2', '338$a', '338$b', '362$a-Start date of publication']\n",
        "\n",
        "for col in columns_to_check:\n",
        "    if col in filtered_columns_df_336.columns:\n",
        "        distinct_values[col] = filtered_columns_df_336[col].dropna().unique()\n",
        "\n",
        "distinct_values_df = pd.DataFrame.from_dict(distinct_values, orient='index').transpose()\n",
        "print(\"\\nDistinct values in specified columns:\")\n",
        "distinct_values_df"
      ],
      "id": "a1d89eb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code shows all variations in entries for 336-338 columns. It’s commented out."
      ],
      "id": "8d7c7cf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 13.4: Display distinct values for each of the specified columns individually\n",
        "# distinct_values_dict = {}\n",
        "# columns_to_check = ['336$b', '337$2', '337$a', '337$b', '338$2', '338$a', '338$b']\n",
        "\n",
        "# for col in columns_to_check:\n",
        "#     if col in filtered_columns_df_336.columns:\n",
        "#         distinct_values_dict[col] = filtered_columns_df_336[col].dropna().unique()\n",
        "\n",
        "# for col, values in distinct_values_dict.items():\n",
        "#     print(f\"Distinct Values in Column '{col}':\")\n",
        "#     for value in values:\n",
        "#         print(value)\n",
        "#     print(\"\\n\")"
      ],
      "id": "b3417556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, I created new columns showing the count of how many records there are according to each previous column because some columns have a contradictory amount of records.\n"
      ],
      "id": "7009e3b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 13.5: Create new columns to count the occurrences of ';' in each of the specified columns\n",
        "columns_to_analyze = ['001-Control Number', '337$a', '337$b', '338$2', '338$a', '338$b']\n",
        "\n",
        "for col in columns_to_analyze:\n",
        "    if col in filtered_columns_df_336.columns:\n",
        "        count_col_name = f\"{col}_count\"\n",
        "        filtered_columns_df_336[count_col_name] = filtered_columns_df_336[col].apply(lambda x: str(x).count(';') if pd.notna(x) else 0)\n",
        "\n",
        "#print(\"\\nFiltered DataFrame with count columns:\")\n",
        "#filtered_columns_df_336"
      ],
      "id": "e816e69b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 14: Create a DataFrame with specified columns\n",
        "final_columns = [\n",
        "    '000-Leader', '336$2', '336$a', '336$b', '337$2', '337$a', '337$b',\n",
        "    '338$2', '338$a', '338$b', '362$a-Start date of publication',\n",
        "    '337$a_count', '337$b_count', '338$2_count', '338$a_count', '338$b_count'\n",
        "]\n",
        "\n",
        "# Add '001-Control Number' only if it is in the columns of filtered_columns_df_336\n",
        "if '001-Control Number' in filtered_columns_df_336.columns:\n",
        "    final_columns.insert(1, '001-Control Number')\n",
        "\n",
        "# Use only the columns that are available in filtered_columns_df_336\n",
        "available_final_columns = [col for col in final_columns if col in filtered_columns_df_336.columns]\n",
        "final_df = filtered_columns_df_336[available_final_columns].copy()\n",
        "\n",
        "print(\"\\nFinal DataFrame with specified columns:\")\n",
        "final_df"
      ],
      "id": "1b06c291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rows filtered so we can see the records that have a contradictary amount of record types. For example, a column may say there are 3 record types, but another column may say there are 4 record types.\n"
      ],
      "id": "5b6c5c2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 13.5: Create new columns to count the occurrences of ';' in each of the specified columns\n",
        "columns_to_analyze = ['001-Control Number', '337$a', '337$b', '338$2', '338$a', '338$b']\n",
        "\n",
        "# Initialize count columns in filtered_columns_df_336 for each analyzed column\n",
        "for col in columns_to_analyze:\n",
        "    if col in filtered_columns_df_336.columns:\n",
        "        count_col_name = f\"{col}_count\"\n",
        "        filtered_columns_df_336[count_col_name] = filtered_columns_df_336[col].apply(lambda x: str(x).count(';') if pd.notna(x) else 0)\n",
        "\n",
        "#print(\"\\nFiltered DataFrame with count columns:\")\n",
        "#filtered_columns_df_336\n",
        "\n",
        "# Add 1 to each cell in the specified count columns\n",
        "count_columns = ['337$a_count', '337$b_count', '338$2_count', '338$a_count', '338$b_count']\n",
        "\n",
        "# Increment the values in each of the specified columns by 1\n",
        "for col in count_columns:\n",
        "    if col in filtered_columns_df_336.columns:\n",
        "        filtered_columns_df_336[col] += 1  # Add 1 to each cell in the count columns\n",
        "\n",
        "# Ensure that '001-Control Number' is included in filtered_columns_df if it exists in df_cleaned\n",
        "if '001-Control Number' in df_cleaned.columns:\n",
        "    filtered_columns_df_336['001-Control Number'] = df_cleaned['001-Control Number']\n",
        "\n",
        "# Define columns to display, including '001-Control Number' and incremented count columns\n",
        "columns_to_display = ['001-Control Number'] + count_columns\n",
        "\n",
        "# Create a condition to filter rows where values are not equal across the count columns (excluding '001-Control Number')\n",
        "unequal_condition = filtered_columns_df_336[count_columns].nunique(axis=1) > 1\n",
        "\n",
        "# Filter rows based on the condition and include '001-Control Number' in the result\n",
        "unequal_rows_df = filtered_columns_df_336[unequal_condition]\n",
        "\n",
        "# Select only the columns that are available in the DataFrame\n",
        "available_columns = [col for col in columns_to_display if col in unequal_rows_df.columns]\n",
        "unequal_rows_df_filtered = unequal_rows_df[available_columns]\n",
        "\n",
        "# Display the resulting DataFrame with unequal count values, including '001-Control Number'\n",
        "unequal_rows_df_filtered"
      ],
      "id": "dc627912",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Extract the unique '001-Control Number' values from unequal_rows_df_filtered\n",
        "control_numbers = unequal_rows_df_filtered['001-Control Number'].unique()\n",
        "\n",
        "# Step 2: Define the columns you want to include in the final table\n",
        "columns_to_retrieve = [\n",
        "    '001-Control Number', '336$2', '336$a', '336$b', '337$2',  '337$a', '337$b', '338$2', '338$a', '338$b',\n",
        "    ]\n",
        "\n",
        "# Step 3: Filter the original DataFrame to retrieve the specified columns\n",
        "# We assume that `filtered_columns_df_336` contains the necessary columns.\n",
        "# If not, replace `filtered_columns_df_336` with the appropriate DataFrame name.\n",
        "\n",
        "# Retrieve rows based on the '001-Control Number' values\n",
        "final_table = filtered_columns_df_336[\n",
        "    filtered_columns_df_336['001-Control Number'].isin(control_numbers)\n",
        "][columns_to_retrieve]\n",
        "\n",
        "# Display the resulting table\n",
        "#final_table"
      ],
      "id": "ed1c4b6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save final_table to a CSV file\n",
        "#final_table.to_csv('problems_resouce_type.csv', index=False)\n",
        "\n",
        "#print(\"final_table has been saved as 'problems_resouce_type.csv'\")"
      ],
      "id": "763d28ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if '336$2' exists in df and count non-null values if it does\n",
        "if '336$2' in df.columns:\n",
        "    non_null_count = df['336$2'].notna().sum()\n",
        "    print(f\"The number of rows with incorrect entries for cound of record types are: {non_null_count}\")\n",
        "else:\n",
        "    print(\"The column '336$2' does not exist in df.\")"
      ],
      "id": "0fab32c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Steps\n",
        "This is all a great start and a solid proof of concept for one potential method for identifying and analyzing different formatting standards that are used in the Family History Library Catalog metadata. This is not a comprehensive analysis, and there is much more to be done. Here are some suggestions for next steps:\n",
        "\n",
        "The next step is to create an application in streamlit so that new data cand be processed automatically and on-demand. This will allow end users to upload newly exported data from KOHA and get specificed results without having to run the code manually. This will also allow for more flexibility and customization of the analysis."
      ],
      "id": "8d7451a2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}